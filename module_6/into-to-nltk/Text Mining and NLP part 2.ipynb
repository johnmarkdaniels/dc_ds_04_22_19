{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining and NLP\n",
    "\n",
    "## Part 2\n",
    "\n",
    "### Situation:\n",
    "\n",
    "Priya works at an international PR firm in the Europe division. Their largest client has offices in Ibiza, Madrid, and Las Palmas. She needs to keep her boss aware of current events and provide a weekly short list of articles concerning political events in Spain. The problem is, this takes hours every week to review articles on the BBC and Priya is very busy! She wonders if she could automate this process using text mining to save her time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Goal**: to internalize the steps, challenges, and methodology of text mining\n",
    "- explore text analysis by hand\n",
    "- apply text mining steps in Jupyter with Python libraries NLTK\n",
    "- classify documents correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refresher on cleaning text\n",
    "![gif](https://www.nyfa.edu/student-resources/wp-content/uploads/2014/10/furious-crazed-typing.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string, re\n",
    "import urllib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# url_a = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/A.txt\"\n",
    "# url_b = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/B.txt\"\n",
    "article_a = urllib.request.urlopen(url_a).read()\n",
    "article_a_st = article_a.decode(\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "url_a = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/A.txt\"\n",
    "url_b = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/B.txt\"\n",
    "url_c = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/C.txt\"\n",
    "url_d = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/D.txt\"\n",
    "url_e = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/E.txt\"\n",
    "url_f = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/F.txt\"\n",
    "url_g = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/G.txt\"\n",
    "url_h = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/H.txt\"\n",
    "url_i = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/I.txt\"\n",
    "url_j = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/J.txt\"\n",
    "url_k = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/K.txt\"\n",
    "url_l = \"https://raw.githubusercontent.com/aapeebles/text_examples/master/Text%20examples%20folder/L.txt\"\n",
    "\n",
    "article_list = [ url_a,url_b,url_c,url_d,url_e,url_f,url_g,url_h,url_i,url_j,url_k,url_l]    \n",
    "\n",
    "article_a = urllib.request.urlopen(url_a).read()\n",
    "article_a_st = article_a.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_code(article):\n",
    "    # tokens\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    art_tokens_raw = nltk.regexp_tokenize(article_a_st, pattern)\n",
    "\n",
    "    # lower case\n",
    "    art_tokens = [i.lower() for i in art_tokens_raw]\n",
    "\n",
    "    # stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords.words(\"english\")\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    art_tokens_stopped = [w for w in art_tokens if not w in stop_words]\n",
    "\n",
    "    # stem words\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    art_stemmed = [stemmer.stem(word) for word in art_tokens_stopped]\n",
    "    \n",
    "    return art_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https',\n",
       "  'raw',\n",
       "  'githubusercont',\n",
       "  'com',\n",
       "  'aapeebl',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'master',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'folder',\n",
       "  'txt'],\n",
       " ['https',\n",
       "  'raw',\n",
       "  'githubusercont',\n",
       "  'com',\n",
       "  'aapeebl',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'master',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'folder',\n",
       "  'b',\n",
       "  'txt'],\n",
       " ['https',\n",
       "  'raw',\n",
       "  'githubusercont',\n",
       "  'com',\n",
       "  'aapeebl',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'master',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'folder',\n",
       "  'c',\n",
       "  'txt'],\n",
       " ['https',\n",
       "  'raw',\n",
       "  'githubusercont',\n",
       "  'com',\n",
       "  'aapeebl',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'master',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'folder',\n",
       "  'txt'],\n",
       " ['https',\n",
       "  'raw',\n",
       "  'githubusercont',\n",
       "  'com',\n",
       "  'aapeebl',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'master',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'folder',\n",
       "  'e',\n",
       "  'txt'],\n",
       " ['https',\n",
       "  'raw',\n",
       "  'githubusercont',\n",
       "  'com',\n",
       "  'aapeebl',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'master',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'folder',\n",
       "  'f',\n",
       "  'txt'],\n",
       " ['https',\n",
       "  'raw',\n",
       "  'githubusercont',\n",
       "  'com',\n",
       "  'aapeebl',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'master',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'folder',\n",
       "  'g',\n",
       "  'txt'],\n",
       " ['https',\n",
       "  'raw',\n",
       "  'githubusercont',\n",
       "  'com',\n",
       "  'aapeebl',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'master',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'folder',\n",
       "  'h',\n",
       "  'txt'],\n",
       " ['https',\n",
       "  'raw',\n",
       "  'githubusercont',\n",
       "  'com',\n",
       "  'aapeebl',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'master',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'folder',\n",
       "  'txt'],\n",
       " ['https',\n",
       "  'raw',\n",
       "  'githubusercont',\n",
       "  'com',\n",
       "  'aapeebl',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'master',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'folder',\n",
       "  'j',\n",
       "  'txt'],\n",
       " ['https',\n",
       "  'raw',\n",
       "  'githubusercont',\n",
       "  'com',\n",
       "  'aapeebl',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'master',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'folder',\n",
       "  'k',\n",
       "  'txt'],\n",
       " ['https',\n",
       "  'raw',\n",
       "  'githubusercont',\n",
       "  'com',\n",
       "  'aapeebl',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'master',\n",
       "  'text',\n",
       "  'exampl',\n",
       "  'folder',\n",
       "  'l',\n",
       "  'txt']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_articles = []\n",
    "for article in article_list:\n",
    "    cleaned_articles.append(cleaning_code(article))\n",
    "    \n",
    "cleaned_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat w second article\n",
    "article_b = urllib.request.urlopen(url_b).read()\n",
    "article_b_st = article_b.decode(\"utf-8\")\n",
    "artb_tokens_raw = nltk.regexp_tokenize(article_b_st, pattern)\n",
    "artb_tokens = [i.lower() for i in artb_tokens_raw]\n",
    "artb_tokens_stopped = [w for w in artb_tokens if not w in stop_words]\n",
    "artb_stemmed = [stemmer.stem(word) for word in artb_tokens_stopped]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what's wrong with the table from yesterday? what does it not consider?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)\n",
    "\n",
    "$\\begin{align}\n",
    " tf_{i,j} = \\dfrac{n_{i,j}}{\\displaystyle \\sum_k n_{i,j} }\n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "$\\begin{align}\n",
    "idf(w) = \\log \\dfrac{N}{df_t}\n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF score\n",
    "\n",
    "$ \\begin{align}\n",
    "w_{i,j} = tf_{i,j} \\times \\log \\dfrac{N}{df_i} \\\\\n",
    "tf_{i,j} = \\text{number of occurences of } i \\text{ in} j \\\\\n",
    "df_i = \\text{number of documents containing} i \\\\\n",
    "N = \\text{total number of documents}\n",
    "\\end{align} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The from scratch method\n",
    "![homemade](https://media2.giphy.com/media/LBZcXdG0eVBdK/giphy.gif?cid=3640f6095c2d7bb2526a424a4d97117c)\n",
    "\n",
    "\n",
    "Please go through the code and comment what each section does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a set of words from the documents without duplicates and dictionaries to count\n",
    "wordSet = set(arta_stemmed).union(set(artb_stemmed))\n",
    "wordDictA = dict.fromkeys(wordSet, 0) \n",
    "wordDictB = dict.fromkeys(wordSet, 0) \n",
    "\n",
    "# Counts words in A\n",
    "for word in arta_stemmed:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "# Counts words in B\n",
    "for word in artb_stemmed:\n",
    "    wordDictB[word]+=1    \n",
    "\n",
    "# Creates a function to return a ratio of each word to all the words in the article\n",
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict\n",
    "\n",
    "\n",
    "tfbowA = computeTF(wordDictA,arta_stemmed)\n",
    "tfbowB = computeTF(wordDictB,artb_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chair': 0.0,\n",
       " 'announc': 0.0,\n",
       " 'financ': 0.0,\n",
       " 'miss': 0.0,\n",
       " 'submit': 0.005434782608695652,\n",
       " 'action': 0.005434782608695652,\n",
       " 'open': 0.005434782608695652,\n",
       " 'draft': 0.016304347826086956,\n",
       " 'one': 0.016304347826086956,\n",
       " 'reach': 0.0,\n",
       " 'fuller': 0.005434782608695652,\n",
       " 'committe': 0.010869565217391304,\n",
       " 'immens': 0.005434782608695652,\n",
       " 'fund': 0.0,\n",
       " 'permit': 0.005434782608695652,\n",
       " 'innov': 0.005434782608695652,\n",
       " 'europ': 0.005434782608695652,\n",
       " 'issu': 0.005434782608695652,\n",
       " 'put': 0.005434782608695652,\n",
       " 'week': 0.0,\n",
       " 'wealthi': 0.0,\n",
       " 'gain': 0.005434782608695652,\n",
       " 'internet': 0.005434782608695652,\n",
       " 'repay': 0.0,\n",
       " 'mr': 0.0,\n",
       " 'biggest': 0.0,\n",
       " 'controversi': 0.005434782608695652,\n",
       " 'similar': 0.005434782608695652,\n",
       " 'program': 0.005434782608695652,\n",
       " 'sourc': 0.005434782608695652,\n",
       " 'ineffici': 0.005434782608695652,\n",
       " 'thursday': 0.0,\n",
       " 'model': 0.005434782608695652,\n",
       " 'welcom': 0.005434782608695652,\n",
       " 'protect': 0.010869565217391304,\n",
       " 'larg': 0.005434782608695652,\n",
       " 'germani': 0.0,\n",
       " 'mep': 0.010869565217391304,\n",
       " 'momentum': 0.005434782608695652,\n",
       " 'thought': 0.0,\n",
       " 'moratorium': 0.0,\n",
       " 'eu': 0.021739130434782608,\n",
       " 'lobbi': 0.005434782608695652,\n",
       " 'chanc': 0.005434782608695652,\n",
       " 'click': 0.005434782608695652,\n",
       " 'legal': 0.016304347826086956,\n",
       " 'serv': 0.005434782608695652,\n",
       " 'suspend': 0.0,\n",
       " 'brown': 0.0,\n",
       " 'setback': 0.005434782608695652,\n",
       " 'say': 0.016304347826086956,\n",
       " 'amazon': 0.005434782608695652,\n",
       " 'direct': 0.021739130434782608,\n",
       " 'give': 0.005434782608695652,\n",
       " 'us': 0.016304347826086956,\n",
       " 'larger': 0.005434782608695652,\n",
       " 'first': 0.005434782608695652,\n",
       " 'affair': 0.005434782608695652,\n",
       " 'intens': 0.005434782608695652,\n",
       " 'talk': 0.0,\n",
       " 'begun': 0.0,\n",
       " 'face': 0.0,\n",
       " 'use': 0.005434782608695652,\n",
       " 'night': 0.0,\n",
       " 'fail': 0.005434782608695652,\n",
       " 'member': 0.010869565217391304,\n",
       " 'adopt': 0.005434782608695652,\n",
       " 'disast': 0.0,\n",
       " 'sign': 0.0,\n",
       " 'method': 0.005434782608695652,\n",
       " 'interest': 0.0,\n",
       " 'poland': 0.005434782608695652,\n",
       " 'court': 0.005434782608695652,\n",
       " 'world': 0.0,\n",
       " 'canada': 0.0,\n",
       " 'complet': 0.0,\n",
       " 'small': 0.010869565217391304,\n",
       " 'happen': 0.005434782608695652,\n",
       " 'monetari': 0.0,\n",
       " 'debt': 0.0,\n",
       " 'gordon': 0.0,\n",
       " 'bn': 0.0,\n",
       " 'month': 0.005434782608695652,\n",
       " 'implement': 0.010869565217391304,\n",
       " 'briton': 0.0,\n",
       " 'state': 0.010869565217391304,\n",
       " 'chancellor': 0.0,\n",
       " 'govern': 0.005434782608695652,\n",
       " 'reject': 0.005434782608695652,\n",
       " 'hurt': 0.005434782608695652,\n",
       " 'lead': 0.005434782608695652,\n",
       " 'countri': 0.0,\n",
       " 'bank': 0.0,\n",
       " 'hold': 0.005434782608695652,\n",
       " 'even': 0.005434782608695652,\n",
       " 'bring': 0.005434782608695652,\n",
       " 'final': 0.0,\n",
       " 'g': 0.0,\n",
       " 'number': 0.0,\n",
       " 'rule': 0.005434782608695652,\n",
       " 'union': 0.005434782608695652,\n",
       " 'nation': 0.005434782608695652,\n",
       " 'softwar': 0.016304347826086956,\n",
       " 'debat': 0.005434782608695652,\n",
       " 'develop': 0.005434782608695652,\n",
       " 'without': 0.005434782608695652,\n",
       " 'read': 0.005434782608695652,\n",
       " 'pound': 0.0,\n",
       " 'intend': 0.005434782608695652,\n",
       " 'start': 0.005434782608695652,\n",
       " 'said': 0.010869565217391304,\n",
       " 'law': 0.02717391304347826,\n",
       " 'base': 0.010869565217391304,\n",
       " 'decis': 0.005434782608695652,\n",
       " 'word': 0.005434782608695652,\n",
       " 'freez': 0.0,\n",
       " 'hope': 0.0,\n",
       " 'group': 0.0,\n",
       " 'year': 0.0,\n",
       " 'effect': 0.005434782608695652,\n",
       " 'also': 0.0,\n",
       " 'shop': 0.005434782608695652,\n",
       " 'twice': 0.005434782608695652,\n",
       " 'line': 0.005434782608695652,\n",
       " 'servic': 0.005434782608695652,\n",
       " 'current': 0.005434782608695652,\n",
       " 'fear': 0.005434782608695652,\n",
       " 'deal': 0.0,\n",
       " 'order': 0.010869565217391304,\n",
       " 'lock': 0.0,\n",
       " 'secretari': 0.0,\n",
       " 'two': 0.010869565217391304,\n",
       " 'might': 0.005434782608695652,\n",
       " 'achiev': 0.005434782608695652,\n",
       " 'largest': 0.005434782608695652,\n",
       " 'believ': 0.0,\n",
       " 'european': 0.010869565217391304,\n",
       " 'earlier': 0.0,\n",
       " 'foreign': 0.0,\n",
       " 'would': 0.016304347826086956,\n",
       " 'straw': 0.0,\n",
       " 'parliament': 0.010869565217391304,\n",
       " 'friday': 0.0,\n",
       " 'concern': 0.005434782608695652,\n",
       " 'busi': 0.005434782608695652,\n",
       " 'impact': 0.005434782608695652,\n",
       " 'intern': 0.0,\n",
       " 'favour': 0.005434782608695652,\n",
       " 'invent': 0.02717391304347826,\n",
       " 'back': 0.010869565217391304,\n",
       " 'commiss': 0.005434782608695652,\n",
       " 'firm': 0.010869565217391304,\n",
       " 'oppon': 0.005434782608695652,\n",
       " 'hit': 0.0,\n",
       " 'affect': 0.0,\n",
       " 'agreement': 0.0,\n",
       " 'analysi': 0.0,\n",
       " 'mean': 0.005434782608695652,\n",
       " 'pressur': 0.005434782608695652,\n",
       " 'come': 0.0,\n",
       " 'comput': 0.021739130434782608,\n",
       " 'agre': 0.0,\n",
       " 'rewrit': 0.005434782608695652,\n",
       " 'reconstruct': 0.0,\n",
       " 'exampl': 0.005434782608695652,\n",
       " 'japan': 0.0,\n",
       " 'creditor': 0.0,\n",
       " 'save': 0.0,\n",
       " 'suffer': 0.005434782608695652,\n",
       " 'dead': 0.0,\n",
       " 'later': 0.0,\n",
       " 'instruct': 0.0,\n",
       " 'implic': 0.005434782608695652,\n",
       " 'could': 0.016304347826086956,\n",
       " 'offer': 0.005434782608695652,\n",
       " 'idea': 0.0,\n",
       " 'meet': 0.005434782608695652,\n",
       " 'financi': 0.005434782608695652,\n",
       " 'fight': 0.005434782608695652,\n",
       " 'britain': 0.0,\n",
       " 'juri': 0.010869565217391304,\n",
       " 'reboot': 0.005434782608695652,\n",
       " 'hammer': 0.0,\n",
       " 'compani': 0.005434782608695652,\n",
       " 'tsunami': 0.0,\n",
       " 'minist': 0.0,\n",
       " 'vote': 0.005434782608695652,\n",
       " 'play': 0.005434782608695652,\n",
       " 'jack': 0.0,\n",
       " 'patent': 0.02717391304347826,\n",
       " 'problem': 0.0,\n",
       " 'abstain': 0.005434782608695652,\n",
       " 'expect': 0.0,\n",
       " 'new': 0.010869565217391304,\n",
       " 'critic': 0.010869565217391304,\n",
       " 'propos': 0.010869565217391304,\n",
       " 'support': 0.010869565217391304,\n",
       " 'let': 0.005434782608695652,\n",
       " 'com': 0.005434782608695652,\n",
       " 'vocal': 0.005434782608695652,\n",
       " 'field': 0.005434782608695652}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfbowA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    ''' Computer inverse document frequency for each document in the doclist\n",
    "    returns: IDF for each document\n",
    "    '''\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    # Create dictionary with the keys from the list of dictionaries and zero values\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    # For each pair in docList, compare to zero and add to dictionary if zero and increment by one.\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    # Computer the TF-IDF (natural) log of the number of \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    '''Creates function for calculating TFIDF'''\n",
    "    tfidf = {} # creates empty dictoinary\n",
    "    for word, val in tfBow.items(): # starts a for loop using keys (word) and values from tfBow\n",
    "        tfidf[word] = val*idfs[word] # for each word in tfBow, the value is multiplied by the idfs for that word\n",
    "                                     # The word and resluting computation are then added to the dictionary tfidf\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfBowA = computeTFIDF(tfbowA, idfs)\n",
    "tfidfBowB = computeTFIDF(tfbowB, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstain</th>\n",
       "      <th>achiev</th>\n",
       "      <th>action</th>\n",
       "      <th>adopt</th>\n",
       "      <th>affair</th>\n",
       "      <th>affect</th>\n",
       "      <th>agre</th>\n",
       "      <th>agreement</th>\n",
       "      <th>also</th>\n",
       "      <th>amazon</th>\n",
       "      <th>...</th>\n",
       "      <th>vocal</th>\n",
       "      <th>vote</th>\n",
       "      <th>wealthi</th>\n",
       "      <th>week</th>\n",
       "      <th>welcom</th>\n",
       "      <th>without</th>\n",
       "      <th>word</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.005845</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    abstain    achiev    action     adopt    affair    affect      agre  \\\n",
       "0  0.001636  0.001636  0.001636  0.001636  0.001636  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.002923  0.002923   \n",
       "\n",
       "   agreement      also    amazon    ...        vocal      vote   wealthi  \\\n",
       "0   0.000000  0.000000  0.001636    ...     0.001636  0.001636  0.000000   \n",
       "1   0.002923  0.005845  0.000000    ...     0.000000  0.000000  0.002923   \n",
       "\n",
       "       week    welcom   without      word     world  would      year  \n",
       "0  0.000000  0.001636  0.001636  0.001636  0.000000    0.0  0.000000  \n",
       "1  0.002923  0.000000  0.000000  0.000000  0.002923    0.0  0.002923  \n",
       "\n",
       "[2 rows x 201 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame([tfidfBowA, tfidfBowB])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But yes, there is an easier way\n",
    "\n",
    "![big deal](https://media0.giphy.com/media/xUA7aQOxkz00lvCAOQ/giphy.gif?cid=3640f6095c2d7c51772f47644d09cc8b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    abstain    achiev    action     adopt    affair    affect      agre  \\\n",
      "0  0.053285  0.053285  0.053285  0.053285  0.053285  0.000000  0.000000   \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.084167  0.084167   \n",
      "\n",
      "   agreement      also    amazon    ...        vocal      vote   wealthi  \\\n",
      "0   0.000000  0.000000  0.053285    ...     0.053285  0.053285  0.000000   \n",
      "1   0.084167  0.168334  0.000000    ...     0.000000  0.000000  0.084167   \n",
      "\n",
      "       week    welcom   without      word     world     would      year  \n",
      "0  0.000000  0.053285  0.053285  0.053285  0.000000  0.113738  0.000000  \n",
      "1  0.084167  0.000000  0.000000  0.000000  0.084167  0.059885  0.084167  \n",
      "\n",
      "[2 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "# create a string again\n",
    "cleaned_a = ' '.join(arta_stemmed)\n",
    "cleaned_b = ' '.join(artb_stemmed)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "response = tfidf.fit_transform([cleaned_a, cleaned_b])\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(response.toarray(), columns=tfidf.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Statistics \n",
    "\n",
    "How many non-zero elements are there?\n",
    "- Adapt the code below, using the `df` version of the `response` object to replace everywhere below it says `DATA`\n",
    "- Interpret the findings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Non-Zero Elements in Vectorized Articles: 103.5\n",
      "Percentage of columns containing 0: 0.48250000000000004\n"
     ]
    }
   ],
   "source": [
    "# Edit code before running it\n",
    "import numpy as np\n",
    "\n",
    "newval = np.array(df)\n",
    "\n",
    "non_zero_vals = np.count_nonzero(newval) / float(df.shape[0])\n",
    "print(\"Average Number of Non-Zero Elements in Vectorized Articles: {}\".format(non_zero_vals))\n",
    "\n",
    "percent_sparse = 1 - (non_zero_cols / float(df.shape[1]))\n",
    "print('Percentage of columns containing 0: {}'.format(percent_sparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps:\n",
    "- Create the tf-idf for the **whole** corpus of 12 articles\n",
    "- What are _on average_ the most important words in the whole corpus?\n",
    "- Add a column named \"Target\" to the dataset\n",
    "- Target will be set to 1 or 0 if the article is \"Politics\" or \"Not Politics\"\n",
    "- Do some exploratory analysis of the dataset\n",
    " - what are the average most important words for the \"Politics\" articles?\n",
    " - What are the average most important words for the \"Not Politics\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets talk classification\n",
    "- How would you split into train and test? what would be the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
